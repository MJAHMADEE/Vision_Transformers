# -*- coding: utf-8 -*-
"""MJAhmadi_NNDL_HW5_Q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GQrqxlaq7CESXTwrdkpYbO7_vT9sRV37

# 2.1. Load and Process Dataset
"""

# Install the required libraries
!pip install torch torchvision datasets transformers

import os
import torch
import torchvision
import torchvision.transforms as transforms

# Define the transformations
upscale_transform = transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR)

# Download and load the CIFAR-10 dataset
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Specify the new root directory for the resized dataset
new_root = './resized_data'

# Create the new directory if it doesn't exist
if not os.path.exists(new_root):
    os.makedirs(new_root)

# Save the resized dataset in the new root directory
torch.save(train_dataset, os.path.join(new_root, 'train_dataset.pth'))
torch.save(test_dataset, os.path.join(new_root, 'test_dataset.pth'))

mean = np.array([0.485, 0.456, 0.406])
std = np.array([0.229, 0.224, 0.225])

data_transforms_train = transforms.Compose(
    [
        transforms.RandomResizedCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean, std),
    ]
)
data_transforms_test = transforms.Compose(
    [
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean, std),
    ]
)

train_dataset = torchvision.datasets.CIFAR10(
    root="./data", train=True, download=True, transform=data_transforms_train
)

test_dataset = torchvision.datasets.CIFAR10(
    root="./data", train=False, download=True, transform=data_transforms_test
)

class_names = train_dataset.classes

"""# 2.2. CNN"""

import torch
import torchvision
import torchvision.transforms as transforms
import torchvision.models as models
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
import warnings
warnings.filterwarnings('ignore')

# Define the transformations
upscale_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()  # Add this line to convert images to tensors
])

# Download and load the CIFAR-10 dataset
train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Define the dataloaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the pre-trained VGG-19 model
model = models.vgg19(pretrained=True)

# Unfreeze layers starting from 'block5_conv1'
unfreeze_from_layer = 'block5_conv1'
unfreeze = False

for name, param in model.named_parameters():
    if unfreeze:
        param.requires_grad = True
    if name == unfreeze_from_layer:
        unfreeze = True

# Fine-tune the model
criterion = torch.nn.CrossEntropyLoss()
initial_lr = 0.0001
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=initial_lr)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 20
lr_factor = 0.6
lr_patience = 1
min_lr = 0.0000001

train_losses = []
train_accs = []
val_losses = []
val_accs = []
lr_values = []

best_val_acc = 0.0
stop_after_epochs = 2
epochs_without_improvement = 0
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=lr_factor, patience=lr_patience,
                                                           min_lr=min_lr, verbose=True)

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        train_acc += (predicted == labels).sum().item()

    train_loss /= len(train_loader.dataset)
    train_acc /= len(train_loader.dataset)

    train_losses.append(train_loss)
    train_accs.append(train_acc)

    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_acc += (predicted == labels).sum().item()

            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())

    val_loss /= len(test_loader.dataset)
    val_acc /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accs.append(val_acc)

    lr_values.append(optimizer.param_groups[0]['lr'])
    lr_scheduler.step(val_acc)

    print(f"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, LR: {optimizer.param_groups[0]['lr']:.8f}")

    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1
        if epochs_without_improvement >= stop_after_epochs:
            print(f"Validation accuracy has not improved for {stop_after_epochs} epochs. "
                  f"Stopping training...")
            break

# Plot val/train accuracy and loss
plt.figure()
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('lossplot22.pdf')
plt.show()

plt.figure()
plt.plot(train_accs, label='Train Accuracy')
plt.plot(val_accs, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('accuracyplot22.pdf')
plt.show()

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.savefig('confusionmatrix22.pdf')
plt.show()

# Print F1-score, recall, accuracy, and precision for all classes
f1_scores = f1_score(y_true, y_pred, average=None)
recall_scores = recall_score(y_true, y_pred, average=None)
accuracy = accuracy_score(y_true, y_pred)
precision_scores = precision_score(y_true, y_pred, average=None)

for i in range(len(f1_scores)):
    print(f"Class {i}: F1-Score: {f1_scores[i]:.4f}, Recall: {recall_scores[i]:.4f}, "
          f"Precision: {precision_scores[i]:.4f}")

print(f"Overall Accuracy: {accuracy:.4f}")

"""# 2.3. Transformers"""

!pip install timm

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
import timm

# Define the transformations
upscale_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()  # Convert images to tensors
])

# Download and load the CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Define the dataloaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the pre-trained ViT-L32 model
model = timm.create_model('vit_large_patch32_224', pretrained=True, num_classes=10)

# Unfreeze layers starting from 'Transformer/encoderblock 23'
unfreeze_from_layer = 'Transformer/encoderblock_23'
unfreeze = False

for name, param in model.named_parameters():
    if unfreeze:
        param.requires_grad = True
    if name.startswith(unfreeze_from_layer):
        unfreeze = True

# Fine-tune the model
criterion = torch.nn.CrossEntropyLoss()
learning_rate = 0.0001
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, patience=1, min_lr=1e-7)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 20
stop_after_epochs = 2  # Stop training after 5 epochs without improvement
best_val_acc = 0.0
epochs_without_improvement = 0

train_losses = []
train_accs = []
val_losses = []
val_accs = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        train_acc += (predicted == labels).sum().item()

    train_loss /= len(train_loader.dataset)
    train_acc /= len(train_loader.dataset)

    train_losses.append(train_loss)
    train_accs.append(train_acc)

    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_acc += (predicted == labels).sum().item()

            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())

    val_loss /= len(test_loader.dataset)
    val_acc /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Check if validation accuracy has improved
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1

    # Reduce learning rate if no improvement for a certain number of epochs
    if epochs_without_improvement >= stop_after_epochs:
        lr_scheduler.step(val_loss)
        if optimizer.param_groups[0]['lr'] < 1e-7:
            print("Training stopped as learning rate reached the minimum value.")
            break

# Plot val/train accuracy and loss
plt.figure()
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('lossplot.pdf')
plt.show()

plt.figure()
plt.plot(train_accs, label='Train Accuracy')
plt.plot(val_accs, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('accuracyplot.pdf')
plt.show()

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.savefig('confusionmatrix.pdf')
plt.show()

# Print F1-score, recall, accuracy, and precision for all classes
f1_scores = f1_score(y_true, y_pred, average=None)
recall_scores = recall_score(y_true, y_pred, average=None)
accuracy = accuracy_score(y_true, y_pred)
precision_scores = precision_score(y_true, y_pred, average=None)

for i in range(len(f1_scores)):
    print(f"Class {i}: F1-Score: {f1_scores[i]:.4f}, Recall: {recall_scores[i]:.4f}, "
          f"Precision: {precision_scores[i]:.4f}")

print(f"Overall Accuracy: {accuracy:.4f}")

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
import timm

# Define the transformations
upscale_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()  # Convert images to tensors
])

# Download and load the CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Define the dataloaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the pre-trained ViT-L32 model
model = timm.create_model('vit_large_patch32_224', pretrained=True, num_classes=10)

# Freeze all layers except the last block of the transformer and the MLP head
for name, param in model.named_parameters():
    if not name.startswith('Transformer/encoderblock_23') and not name.startswith('head'):
        param.requires_grad = False

# Fine-tune the model
criterion = torch.nn.CrossEntropyLoss()
learning_rate = 0.0001
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, patience=1, min_lr=1e-7)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 20
stop_after_epochs = 2  # Stop training after 5 epochs without improvement
best_val_acc = 0.0
epochs_without_improvement = 0

train_losses = []
train_accs = []
val_losses = []
val_accs = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        train_acc += (predicted == labels).sum().item()

    train_loss /= len(train_loader.dataset)
    train_acc /= len(train_loader.dataset)

    train_losses.append(train_loss)
    train_accs.append(train_acc)

    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_acc += (predicted == labels).sum().item()

            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())

    val_loss /= len(test_loader.dataset)
    val_acc /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Check if validation accuracy has improved
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1

    # Reduce learning rate if no improvement for a certain number of epochs
    if epochs_without_improvement >= stop_after_epochs:
        lr_scheduler.step(val_loss)
        if optimizer.param_groups[0]['lr'] < 1e-7:
            print("Training stopped as learning rate reached the minimum value.")
            break

# Plot val/train accuracy and loss
plt.figure()
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('lossplot33.pdf')
plt.show()

plt.figure()
plt.plot(train_accs, label='Train Accuracy')
plt.plot(val_accs, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('accuracyplot33.pdf')
plt.show()

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.savefig('confusionmatrix33.pdf')
plt.show()

# Print F1-score, recall, accuracy, and precision for all classes
f1_scores = f1_score(y_true, y_pred, average=None)
recall_scores = recall_score(y_true, y_pred, average=None)
accuracy = accuracy_score(y_true, y_pred)
precision_scores = precision_score(y_true, y_pred, average=None)

for i in range(len(f1_scores)):
    print(f"Class {i}: F1-Score: {f1_scores[i]:.4f}, Recall: {recall_scores[i]:.4f}, "
          f"Precision: {precision_scores[i]:.4f}")

print(f"Overall Accuracy: {accuracy:.4f}")

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
import timm

# Define the transformations
upscale_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()  # Convert images to tensors
])

# Download and load the CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Define the dataloaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the pre-trained CaiT-S24 model
model = timm.create_model('cait_s24_224', pretrained=True, num_classes=10)

# Freeze all layers except the last block of the transformer and the MLP head
for name, param in model.named_parameters():
    if not name.startswith('blocks.11') and not name.startswith('head'):
        param.requires_grad = False

# Fine-tune the model
criterion = torch.nn.CrossEntropyLoss()
learning_rate = 0.0001
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, patience=1, min_lr=1e-7)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 20
stop_after_epochs = 2  # Stop training after 5 epochs without improvement
best_val_acc = 0.0
epochs_without_improvement = 0

train_losses = []
train_accs = []
val_losses = []
val_accs = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        train_acc += (predicted == labels).sum().item()

    train_loss /= len(train_loader.dataset)
    train_acc /= len(train_loader.dataset)

    train_losses.append(train_loss)
    train_accs.append(train_acc)

    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_acc += (predicted == labels).sum().item()

            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())

    val_loss /= len(test_loader.dataset)
    val_acc /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Check if validation accuracy has improved
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1

    # Reduce learning rate if no improvement for a certain number of epochs
    if epochs_without_improvement >= stop_after_epochs:
        lr_scheduler.step(val_loss)
        if optimizer.param_groups[0]['lr'] < 1e-7:
            print("Training stopped as learning rate reached the minimum value.")
            break

# Plot val/train accuracy and loss
plt.figure()
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('44lossplot.pdf')
plt.show()

plt.figure()
plt.plot(train_accs, label='Train Accuracy')
plt.plot(val_accs, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('44accuracyplot.pdf')
plt.show()

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.savefig('44confusionmatrix.pdf')
plt.show()

# Print F1-score, recall, accuracy, and precision for all classes
f1_scores = f1_score(y_true, y_pred, average=None)
recall_scores = recall_score(y_true, y_pred, average=None)
accuracy = accuracy_score(y_true, y_pred)
precision_scores = precision_score(y_true, y_pred, average=None)

for i in range(len(f1_scores)):
    print(f"Class {i}: F1-Score: {f1_scores[i]:.4f}, Recall: {recall_scores[i]:.4f}, "
          f"Precision: {precision_scores[i]:.4f}")

print(f"Overall Accuracy: {accuracy:.4f}")

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
import timm

# Define the transformations
upscale_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()  # Convert images to tensors
])

# Download and load the CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Define the dataloaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Load the pre-trained ViT-L32 model
model = timm.create_model('vit_large_patch32_224', pretrained=True, num_classes=10)

# Freeze all layers except the last block of the transformer and the MLP head
for name, param in model.named_parameters():
    if not name.startswith('Transformer/encoderblock_23') and not name.startswith('head'):
        param.requires_grad = False

# Fine-tune the model
criterion = torch.nn.CrossEntropyLoss()
learning_rate = 0.0001
optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=learning_rate)
lr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.6, patience=1, min_lr=1e-7)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

num_epochs = 30
stop_after_epochs = 2  # Stop training after 5 epochs without improvement
best_val_acc = 0.0
epochs_without_improvement = 0

train_losses = []
train_accs = []
val_losses = []
val_accs = []

for epoch in range(num_epochs):
    model.train()
    train_loss = 0.0
    train_acc = 0.0

    for images, labels in train_loader:
        images = images.to(device)
        labels = labels.to(device)

        optimizer.zero_grad()
        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        train_loss += loss.item()
        _, predicted = torch.max(outputs.data, 1)
        train_acc += (predicted == labels).sum().item()

    train_loss /= len(train_loader.dataset)
    train_acc /= len(train_loader.dataset)

    train_losses.append(train_loss)
    train_accs.append(train_acc)

    model.eval()
    val_loss = 0.0
    val_acc = 0.0
    y_true = []
    y_pred = []

    with torch.no_grad():
        for images, labels in test_loader:
            images = images.to(device)
            labels = labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)
            val_loss += loss.item()
            _, predicted = torch.max(outputs.data, 1)
            val_acc += (predicted == labels).sum().item()

            y_true.extend(labels.tolist())
            y_pred.extend(predicted.tolist())

    val_loss /= len(test_loader.dataset)
    val_acc /= len(test_loader.dataset)

    val_losses.append(val_loss)
    val_accs.append(val_acc)

    print(f"Epoch {epoch + 1}/{num_epochs}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, "
          f"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}")

    # Check if validation accuracy has improved
    if val_acc > best_val_acc:
        best_val_acc = val_acc
        epochs_without_improvement = 0
    else:
        epochs_without_improvement += 1

    # Reduce learning rate if no improvement for a certain number of epochs
    if epochs_without_improvement >= stop_after_epochs:
        lr_scheduler.step(val_loss)
        if optimizer.param_groups[0]['lr'] < 1e-7:
            print("Training stopped as learning rate reached the minimum value.")
            break

# Plot val/train accuracy and loss
plt.figure()
plt.plot(train_losses, label='Train Loss')
plt.plot(val_losses, label='Val Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.savefig('lossplot33.pdf')
plt.show()

plt.figure()
plt.plot(train_accs, label='Train Accuracy')
plt.plot(val_accs, label='Val Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('accuracyplot33.pdf')
plt.show()

# Calculate and plot confusion matrix
cm = confusion_matrix(y_true, y_pred)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap='Blues', cbar=False)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.savefig('confusionmatrix33.pdf')
plt.show()

# Print F1-score, recall, accuracy, and precision for all classes
f1_scores = f1_score(y_true, y_pred, average=None)
recall_scores = recall_score(y_true, y_pred, average=None)
accuracy = accuracy_score(y_true, y_pred)
precision_scores = precision_score(y_true, y_pred, average=None)

for i in range(len(f1_scores)):
    print(f"Class {i}: F1-Score: {f1_scores[i]:.4f}, Recall: {recall_scores[i]:.4f}, "
          f"Precision: {precision_scores[i]:.4f}")

print(f"Overall Accuracy: {accuracy:.4f}")

import torch
import torchvision.transforms as transforms
import torchvision.datasets as datasets
import matplotlib.pyplot as plt
from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns
import numpy as np
from sklearn.metrics import f1_score, recall_score, accuracy_score, precision_score
import timm

# Define the transformations
upscale_transform = transforms.Compose([
    transforms.Resize((224, 224), interpolation=transforms.InterpolationMode.BILINEAR),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor()  # Convert images to tensors
])

# Download and load the CIFAR-10 dataset
train_dataset = datasets.CIFAR10(root='./data', train=True, transform=upscale_transform, download=True)
test_dataset = datasets.CIFAR10(root='./data', train=False, transform=upscale_transform, download=True)

# Define the dataloaders
batch_size = 64
train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)

# Fine-tune CaiT-S24 model for classification
model = timm.create_model('cait_s24_224', pretrained=True, num_classes=10)
criterion = torch.nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

epochs = 10
train_loss = []
train_acc = []
val_loss = []
val_acc = []

for epoch in range(epochs):
    # Training loop
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)
        optimizer.zero_grad()

        outputs = model(images)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        _, predicted = torch.max(outputs.data, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

        running_loss += loss.item()

    train_loss.append(running_loss / len(train_loader))
    train_acc.append(correct / total)

    # Validation loop
    model.eval()
    running_loss = 0.0
    correct = 0
    total = 0

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(device), labels.to(device)

            outputs = model(images)
            loss = criterion(outputs, labels)

            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

            running_loss += loss.item()

        val_loss.append(running_loss / len(test_loader))
        val_acc.append(correct / total)

    print(f"Epoch [{epoch+1}/{epochs}], "
          f"Train Loss: {train_loss[-1]:.4f}, Train Acc: {train_acc[-1]:.4f}, "
          f"Val Loss: {val_loss[-1]:.4f}, Val Acc: {val_acc[-1]:.4f}")

# Plot loss and accuracy for train/validation
plt.figure()
plt.plot(range(1, epochs+1), train_loss, label='Train Loss')
plt.plot(range(1, epochs+1), val_loss, label='Val Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.savefig('loss_plot.pdf')
plt.close()

plt.figure()
plt.plot(range(1, epochs+1), train_acc, label='Train Acc')
plt.plot(range(1, epochs+1), val_acc, label='Val Acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.savefig('accuracy_plot.pdf')
plt.close()

# Calculate predictions and metrics
model.eval()
predictions = []
true_labels = []

with torch.no_grad():
    for images, labels in test_loader:
        images, labels = images.to(device), labels.to(device)

        outputs = model(images)
        _, predicted = torch.max(outputs.data, 1)

        predictions.extend(predicted.tolist())
        true_labels.extend(labels.tolist())

# Plot confusion matrix
cm = confusion_matrix(true_labels, predictions)
plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, cmap="Blues", fmt="d", xticklabels=train_dataset.classes, yticklabels=train_dataset.classes)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.savefig('confusion_matrix.pdf')
plt.close()

# Calculate and print metrics
f1 = f1_score(true_labels, predictions, average='weighted')
recall = recall_score(true_labels, predictions, average='weighted')
precision = precision_score(true_labels, predictions, average='weighted')
accuracy = accuracy_score(true_labels, predictions)

print(f"F1 Score: {f1:.4f}")
print(f"Recall: {recall:.4f}")
print(f"Precision: {precision:.4f}")
print(f"Accuracy: {accuracy:.4f}")